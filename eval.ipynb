{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk pandas\n",
    "%pip install textstat\n",
    "%pip install transformers torch scikit-learn\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('human_res.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedback word count\n",
    "df['feedback'] = df['feedback'].astype(str)\n",
    "df['wc'] = df['feedback'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feedback sentence count\n",
    "def count_sentences(feedback):\n",
    "    sentences = sent_tokenize(feedback)\n",
    "    return len(sentences)\n",
    "\n",
    "df['sc'] = df['feedback'].apply(count_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fleisch kincaid\n",
    "import pandas as pd\n",
    "import textstat\n",
    "df['fk'] = df['feedback'].apply(textstat.flesch_reading_ease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTR\n",
    "import nltk\n",
    "def calculate_ttr(feedback):\n",
    "    tokens = word_tokenize(feedback)\n",
    "    unique_tokens = set(tokens)\n",
    "    ttr = len(unique_tokens) / len(tokens) if tokens else 0\n",
    "    return ttr\n",
    "\n",
    "df['ttr'] = df['feedback'].apply(calculate_ttr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse perplexity\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Function to calculate inverse perplexity\n",
    "def calculate_inverse_perplexity(feedback):\n",
    "    inputs = tokenizer(feedback, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "    loss = outputs.loss.item()\n",
    "    perplexity = torch.exp(torch.tensor(loss)).item()\n",
    "    inverse_perplexity = 1 / perplexity if perplexity != 0 else 0\n",
    "    return inverse_perplexity\n",
    "\n",
    "df['inverse_perplexity'] = df['feedback'].apply(calculate_inverse_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Word Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def calculate_content_word_density(feedback):\n",
    "    tokens = word_tokenize(feedback)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    # Content words are nouns, verbs, adjectives, and adverbs\n",
    "    content_words = [word for word, pos in pos_tags if pos.startswith('NN') or pos.startswith('VB') or pos.startswith('JJ') or pos.startswith('RB')]\n",
    "    content_word_density = len(content_words) / len(tokens) if tokens else 0\n",
    "    \n",
    "    return content_word_density\n",
    "\n",
    "df['content_word_density'] = df['feedback'].apply(calculate_content_word_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uptake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def. uptake as overlap\n",
    "def calculate_uptake(excerpt, feedback):\n",
    "    # Tokenize both excerpt and feedback comment\n",
    "    excerpt_tokens = set(word_tokenize(excerpt.lower()))\n",
    "    feedback_tokens = set(word_tokenize(feedback.lower()))\n",
    "    \n",
    "    # Find common words\n",
    "    common_words = excerpt_tokens.intersection(feedback_tokens)\n",
    "    \n",
    "    # Calculate uptake as the ratio of common words to total words in the excerpt\n",
    "    uptake = len(common_words) / len(excerpt_tokens) if excerpt_tokens else 0\n",
    "    \n",
    "    return uptake\n",
    "\n",
    "# Apply the function to calculate uptake for each row\n",
    "df['uptake'] = df.apply(lambda row: calculate_uptake(row['excerpt'], row['feedback']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# cosine similarity of embeddings\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Function to calculate cosine similarity between embeddings\n",
    "def calculate_cosine_similarity(excerpt, feedback):\n",
    "    excerpt_embedding = get_embeddings(excerpt)\n",
    "    feedback_embedding = get_embeddings(feedback)\n",
    "    similarity = cosine_similarity([excerpt_embedding], [feedback_embedding])[0][0]\n",
    "    return similarity\n",
    "\n",
    "# Apply the function to calculate cosine similarity for each row\n",
    "df['cosine_similarity'] = df.apply(lambda row: calculate_cosine_similarity(row['excerpt'], row['feedback']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Focus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_person_singular = {'i', 'me', 'my', 'mine'}\n",
    "second_person = {'you', 'your', 'yours'}\n",
    "first_person_plural = {'we', 'us', 'our', 'ours'}\n",
    "\n",
    "def count_pronouns(feedback, pronoun_set):\n",
    "    tokens = word_tokenize(feedback.lower())\n",
    "    pronoun_count = sum(1 for token in tokens if token in pronoun_set)\n",
    "    return pronoun_count\n",
    "\n",
    "df['pronoun_fps'] = df['feedback'].apply(lambda x: count_pronouns(x, first_person_singular))\n",
    "df['pronoun_sp'] = df['feedback'].apply(lambda x: count_pronouns(x, second_person))\n",
    "df['pronoun_fpp'] = df['feedback'].apply(lambda x: count_pronouns(x, first_person_plural))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_words = {'what', 'where', 'when', 'which', 'who', 'whom', 'whose', 'why', 'how'}\n",
    "auxiliary_verbs = {'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'shall', 'should', 'can', 'could', 'may', 'might', 'must'}\n",
    "helping_verbs = {\"is\", \"am\", \"can\", \"are\", \"do\", \"does\"}\n",
    "\n",
    "def is_question(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return sentence.strip().endswith('?') or (tokens and (tokens[0] in wh_words or tokens[0] in helping_verbs))\n",
    "\n",
    "def count_wh_questions(feedback):\n",
    "    sentences = sent_tokenize(feedback.lower())\n",
    "    wh_question_count = sum(1 for sentence in sentences if is_question(sentence) and word_tokenize(sentence)[0] in wh_words)\n",
    "    return wh_question_count\n",
    "\n",
    "def count_yes_no_questions(feedback):\n",
    "    sentences = sent_tokenize(feedback.lower())\n",
    "    yes_no_question_count = sum(1 for sentence in sentences if is_question(sentence) and word_tokenize(sentence)[0] in auxiliary_verbs)\n",
    "    return yes_no_question_count\n",
    "\n",
    "df['wh_question_count'] = df['feedback'].apply(count_wh_questions)\n",
    "df['yes_no_question_count'] = df['feedback'].apply(count_yes_no_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.to_csv('human_meta_pt1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>grade</th>\n",
       "      <th>prompt</th>\n",
       "      <th>essay</th>\n",
       "      <th>essayid</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>feedback</th>\n",
       "      <th>startidx</th>\n",
       "      <th>endidx</th>\n",
       "      <th>commentid</th>\n",
       "      <th>...</th>\n",
       "      <th>ttr</th>\n",
       "      <th>inverse_perplexity</th>\n",
       "      <th>content_word_density</th>\n",
       "      <th>uptake</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>pronoun_fps</th>\n",
       "      <th>pronoun_sp</th>\n",
       "      <th>pronoun_fpp</th>\n",
       "      <th>wh_question_count</th>\n",
       "      <th>yes_no_question_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rfQv3LPBikeoFIE1NyA3</td>\n",
       "      <td>ms</td>\n",
       "      <td>Some of your friends perform community service...</td>\n",
       "      <td>Dear Principal,\\n\\nI have heard you are having...</td>\n",
       "      <td>1CE6C10B9683</td>\n",
       "      <td>i</td>\n",
       "      <td>Capital and introductory phrase</td>\n",
       "      <td>255</td>\n",
       "      <td>258</td>\n",
       "      <td>1718398386391</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.507323</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rfQv3LPBikeoFIE1NyA3</td>\n",
       "      <td>ms</td>\n",
       "      <td>Some of your friends perform community service...</td>\n",
       "      <td>Dear Principal,\\n\\nI have heard you are having...</td>\n",
       "      <td>1CE6C10B9683</td>\n",
       "      <td>e a</td>\n",
       "      <td>Introductory phrase</td>\n",
       "      <td>548</td>\n",
       "      <td>551</td>\n",
       "      <td>1718398439880</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.654442</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rfQv3LPBikeoFIE1NyA3</td>\n",
       "      <td>ms</td>\n",
       "      <td>Some of your friends perform community service...</td>\n",
       "      <td>Dear Principal,\\n\\nI have heard you are having...</td>\n",
       "      <td>1CE6C10B9683</td>\n",
       "      <td>dyer</td>\n",
       "      <td>wrong word</td>\n",
       "      <td>1137</td>\n",
       "      <td>1141</td>\n",
       "      <td>1718398530582</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.683321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rfQv3LPBikeoFIE1NyA3</td>\n",
       "      <td>ms</td>\n",
       "      <td>Some of your friends perform community service...</td>\n",
       "      <td>Dear Principal,\\n\\nI have heard you are having...</td>\n",
       "      <td>1CE6C10B9683</td>\n",
       "      <td>know it may sound funny but picking up trash ...</td>\n",
       "      <td>run-on sentence</td>\n",
       "      <td>1267</td>\n",
       "      <td>1487</td>\n",
       "      <td>1718398574934</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.426005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rfQv3LPBikeoFIE1NyA3</td>\n",
       "      <td>ms</td>\n",
       "      <td>Some of your friends perform community service...</td>\n",
       "      <td>Dear Principal,\\n\\nI have heard you are having...</td>\n",
       "      <td>1CE6C10B9683</td>\n",
       "      <td>or example if one student did community servic...</td>\n",
       "      <td>Good example</td>\n",
       "      <td>791</td>\n",
       "      <td>875</td>\n",
       "      <td>1718398626223</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.460753</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    tid grade  \\\n",
       "0  rfQv3LPBikeoFIE1NyA3    ms   \n",
       "1  rfQv3LPBikeoFIE1NyA3    ms   \n",
       "2  rfQv3LPBikeoFIE1NyA3    ms   \n",
       "3  rfQv3LPBikeoFIE1NyA3    ms   \n",
       "4  rfQv3LPBikeoFIE1NyA3    ms   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Some of your friends perform community service...   \n",
       "1  Some of your friends perform community service...   \n",
       "2  Some of your friends perform community service...   \n",
       "3  Some of your friends perform community service...   \n",
       "4  Some of your friends perform community service...   \n",
       "\n",
       "                                               essay       essayid  \\\n",
       "0  Dear Principal,\\n\\nI have heard you are having...  1CE6C10B9683   \n",
       "1  Dear Principal,\\n\\nI have heard you are having...  1CE6C10B9683   \n",
       "2  Dear Principal,\\n\\nI have heard you are having...  1CE6C10B9683   \n",
       "3  Dear Principal,\\n\\nI have heard you are having...  1CE6C10B9683   \n",
       "4  Dear Principal,\\n\\nI have heard you are having...  1CE6C10B9683   \n",
       "\n",
       "                                             excerpt  \\\n",
       "0                                                 i    \n",
       "1                                                e a   \n",
       "2                                               dyer   \n",
       "3   know it may sound funny but picking up trash ...   \n",
       "4  or example if one student did community servic...   \n",
       "\n",
       "                          feedback  startidx  endidx      commentid  ...  ttr  \\\n",
       "0  Capital and introductory phrase       255     258  1718398386391  ...  1.0   \n",
       "1              Introductory phrase       548     551  1718398439880  ...  1.0   \n",
       "2                       wrong word      1137    1141  1718398530582  ...  1.0   \n",
       "3                  run-on sentence      1267    1487  1718398574934  ...  1.0   \n",
       "4                     Good example       791     875  1718398626223  ...  1.0   \n",
       "\n",
       "   inverse_perplexity  content_word_density  uptake cosine_similarity  \\\n",
       "0            0.000089                  0.75  0.0000          0.507323   \n",
       "1            0.000053                  1.00  0.0000          0.654442   \n",
       "2            0.000170                  1.00  0.0000          0.683321   \n",
       "3            0.000780                  1.00  0.0000          0.426005   \n",
       "4            0.000219                  1.00  0.0625          0.460753   \n",
       "\n",
       "   pronoun_fps  pronoun_sp  pronoun_fpp  wh_question_count  \\\n",
       "0            0           0            0                  0   \n",
       "1            0           0            0                  0   \n",
       "2            0           0            0                  0   \n",
       "3            0           0            0                  0   \n",
       "4            0           0            0                  0   \n",
       "\n",
       "   yes_no_question_count  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('baseline_meta_pt1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Dialogic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"meiflwr/nondialogic_feedback\", device = 0)\n",
    "def get_dro(excerpt, feedback):\n",
    "    out = []\n",
    "    dro_out = pipe([{'text': feedback, 'text_pair': excerpt}])\n",
    "    if dro_out[0]['label'] == 'LABEL_1':\n",
    "        out.append(1)\n",
    "    else:\n",
    "        out.append(0)\n",
    "    return out\n",
    "\n",
    "df['ND'] = df.apply(lambda row: get_dro(row['excerpt'], row['feedback']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Revision-Oriented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"meiflwr/nonrevisionoriented_feedback\")\n",
    "def get_dro(excerpt, feedback):\n",
    "    out = []\n",
    "    dro_out = pipe([{'text': feedback, 'text_pair': excerpt}])\n",
    "    if dro_out[0]['label'] == 'LABEL_1':\n",
    "        out.append(1)\n",
    "    else:\n",
    "        out.append(0)\n",
    "    return out\n",
    "\n",
    "df['NRO'] = df.apply(lambda row: get_dro(row['excerpt'], row['feedback']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Praise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-classification\", model=\"meiflwr/praise_feedback\")\n",
    "def get_dro(excerpt, feedback):\n",
    "    out = []\n",
    "    dro_out = pipe([{'text': feedback, 'text_pair': excerpt}])\n",
    "    if dro_out[0]['label'] == 'LABEL_1':\n",
    "        out.append(1)\n",
    "    else:\n",
    "        out.append(0)\n",
    "    return out\n",
    "\n",
    "df['PP'] = df.apply(lambda row: get_dro(row['excerpt'], row['feedback']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power-Affirming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe1 = pipeline(\"text-classification\", model=\"meiflwr/poweraffirming_feedback_skew1\")\n",
    "pipe2 = pipeline(\"text-classification\", model=\"meiflwr/poweraffirming_feedback_skew2\")\n",
    "pipe3 = pipeline(\"text-classification\", model=\"meiflwr/poweraffirming_feedback_skew3\")\n",
    "\n",
    "def get_pa(excerpt, feedback, pipeline):\n",
    "  reward = pipeline([{'text': feedback, 'text_pair': excerpt}])\n",
    "  return reward[0]['score']\n",
    "\n",
    "df['pa1'] = df.apply(lambda row: get_pa(row['excerpt'], row['feedback'], pipe1), axis=1)\n",
    "df['pa2'] = df.apply(lambda row: get_pa(row['excerpt'], row['feedback'], pipe2), axis=1)\n",
    "df['pa3'] = df.apply(lambda row: get_pa(row['excerpt'], row['feedback'], pipe3), axis=1)\n",
    "\n",
    "# Calculate the variance of the pa-scores\n",
    "df['pa_var'] = df[['pa1', 'pa2', 'pa3']].var(axis=1)\n",
    "df['pa_mean'] = df[['pa1', 'pa2', 'pa3']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('baseline_res_meta.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
